{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model for the OHSUmed corpus\n",
    "#### Establishing a baseline in Keras\n",
    "\n",
    "The OHSUmed test collection is a subset of the MEDLINE database, which is a bibliographic database of important, peer-reviewed medical literature maintained by the National Library of Medicine. The subset we consider is the collection consisting of the first 20,000 documents from the 50,216 medical abstracts of the year 1991. The classification scheme consists of the 23 Medical Subject Headings (MeSH) categories of cardiovascular diseases group. \n",
    "\n",
    "After selecting such category subset, the document number is 13,924 documents (6,285 for training and 7,649 for testing). Of the 23 categories of the cardiovascular diseases group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from src.models import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "# incorporate only frequent labels\n",
    "def isolate_frequent_labels(X, label_column, threshold_count):\n",
    "    # returns: dataframe with only infrequent labels\n",
    "    df = X.groupby(label_column).size()[X.groupby(label_column).size() > threshold_count].reset_index()\n",
    "    frequent_labels = df.iloc[:,0]\n",
    "    return X[X.label.isin(frequent_labels)]\n",
    "\n",
    "OHSUcsv = pd.read_csv(\"../data/processed/ohsumed_abstracts.csv\", index_col =\"Unnamed: 0\")\n",
    "data = isolate_frequent_labels(OHSUcsv, 'label', 200)\n",
    "#data = OHSUcsv[OHSUcsv.label.isin(frequent_labels)]\n",
    "\n",
    "train_posts = data.loc[data.split == 'train', 'doc']\n",
    "train_tags = data.loc[data.split == 'train', 'label']\n",
    "test_posts = data.loc[data.split == 'test', 'doc']\n",
    "test_tags = data.loc[data.split == 'test', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with Keras\n",
    "### Text Input\n",
    "Keras has some built in methods for preprocessing text to make preprocessing simple. \n",
    "\n",
    "##### Tokenizer class\n",
    "The Tokenizer class provides methods to count the unique words in our vocabulary and assign each of those words to indices. We’ll create an instance of the Tokenizer class, and then pass it the Pandas dataframe of text we want to train on. Although the Tokenizer function, takes a 'num_words' argument to limit the text to a certain vocabulary size (say, 10,000), we will instead use all words. Note that stopwords were already removed.\n",
    "\n",
    "##### fit_on_texts\n",
    "Calling fit_on_texts() automatically creates a word index lookup of our vocabulary, thereby associating each word with a unique number.  \n",
    "\n",
    "##### texts_to_matrix\n",
    "With our Tokenizer, we can now use the texts_to_matrix method to create the training data we’ll pass our model. This will take each post’s text and turn it into a vocab_size “bag” array, with 1s indicating the indices where words in a question are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "tokenize = text.Tokenizer(num_words = vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing output labels\n",
    "The tag for each question is a number (i.e. “1” or “2”). Instead of using a single int as the label for each input, we’ll turn it into a one-hot vector. **We feed a one-hot vector to our model instead of a single integer because the models will output a vector of probabilities for each document** \n",
    "\n",
    "scikit-learn has a **LabelBinarizer class** which makes it easy to build these one-hot vectors. We can pass it the labels column from our Pandas DataFrame and then call fit() and transform() on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple multilayer perception\n",
    "### Using the Sequential Model API in Keras\n",
    "To define the layers of our model we’ll use the Keras **Sequential model API**, which composes a linear stack of layers.\n",
    "\n",
    "**1st layer**: The input layer will take the vocab_size arrays for each comment. We’ll specify this as a Dense layer in Keras, which means each neuron in this layer will be fully connected to all neurons in the next layer. We pass the Dense layer two parameters: the dimensionality of the layer’s output (number of neurons) and the shape of our input data. Choosing the number of dimensions requires some experimentation, but most use a power of 2 as the number of dimensions, so we’ll start with 512.\n",
    "\n",
    "**2nd Layer**: The final layer will use the Softmax activation function, which normalizes the evidence for each possible label into a probability (from 0 to 1). For each category, there is a True and False label, so we have 2 units. If we were to allow our model to model the probability of all N (mutually-exclusive) categories, this final layer would have N+1 units. \n",
    "\n",
    "**Training Protocol**: We call the compile method with the loss function we want to use, the type of optimizer, and the metrics our model should evaluate during training and testing. We’ll use the cross entropy loss function, since each of our abstrats can only belong to one post. The optimizer is the function our model uses to minimize loss. In this example we’ll use the Adam optimizer. There are many optimizers available, all of which are different implementations of gradient descent. For metrics we’ll evaluate accuracy, which will tell us the percentage of abstracts assigned the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Assigning a Cost Function and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we’ll call the fit() method, pass it our training data and labels, the number of examples to process in each batch (batch size), how many times the model should train on our entire dataset (epochs), and the validation split. validation_split tells Keras what percentage of our training data to reserve for validation.\n",
    "\n",
    "**Try tweaking these hyperparameters when using this model on our own data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5465 samples, validate on 608 samples\n",
      "Epoch 1/1\n",
      "5465/5465 [==============================] - 7s 1ms/step - loss: 0.3635 - binary_accuracy: 0.4919 - val_loss: 0.0972 - val_binary_accuracy: 0.4688\n",
      "7367/7367 [==============================] - 1s 150us/step\n"
     ]
    }
   ],
   "source": [
    "# try to predict only the first label\n",
    "column_index = 1\n",
    "\n",
    "label_train = y_train[:,column_index]\n",
    "history = model.fit(x_train,  label_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=1, \n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "y_pred = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returns: dataframe with precision, recall, and F\n",
      "   precision    recall        f1\n",
      "0   0.001735  0.021739  0.003213\n"
     ]
    }
   ],
   "source": [
    "print(metrics.get_binary_results(y_pred, y_test, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3514701e-09"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7367"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
